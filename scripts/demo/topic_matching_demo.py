#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŸºäºä¸»é¢˜å‘ç°çš„ç”¨æˆ·åŒ¹é…ç³»ç»Ÿ - å®é™…ä½¿ç”¨ç¤ºä¾‹

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨ä½ çš„å®é™…åœºæ™¯ä¸­ä½¿ç”¨ä¸»é¢˜å‘ç°åŒ¹é…ç³»ç»Ÿï¼š
1. ä½¿ç”¨ç°æœ‰ç”¨æˆ·æ•°æ®è®­ç»ƒæ¨¡å‹
2. æ‰¹é‡å¤„ç†ç”¨æˆ·ç”Ÿæˆå‘é‡
3. å®æ—¶åŒ¹é…æ–°ç”¨æˆ·
4. è¯„ä¼°åŒ¹é…æ•ˆæœ
"""

import json
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from backend.models.topic_modeling import LDATopicModel
from backend.models.vector_matching import TopicVectorizer, VectorUserMatcher
from typing import List, Dict, Any

class ProductionMatchingSystem:
    """ç”Ÿäº§ç¯å¢ƒåŒ¹é…ç³»ç»Ÿ"""
    
    def __init__(self, model_path: str = "data/models/production_model"):
        self.model_path = model_path
        self.vectorizer = None
        self.matcher = None
        self.is_trained = False
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        os.makedirs("data/user_vectors", exist_ok=True)
    
    def train_from_existing_users(self, users_data: List[Dict[str, Any]]) -> None:
        """ä»ç°æœ‰ç”¨æˆ·æ•°æ®è®­ç»ƒæ¨¡å‹"""
        
        print("ğŸ”„ å¼€å§‹è®­ç»ƒä¸»é¢˜æ¨¡å‹...")
        
        # æå–æ‰€æœ‰ç”¨æˆ·æ–‡æœ¬ç”¨äºè®­ç»ƒ
        training_texts = [user['text'] for user in users_data if user.get('text')]
        
        if len(training_texts) < 10:
            print("âš ï¸  è­¦å‘Šï¼šè®­ç»ƒæ•°æ®å¤ªå°‘ï¼Œå»ºè®®è‡³å°‘50ä¸ªç”¨æˆ·æ–‡æœ¬")
        
        # æ ¹æ®æ•°æ®é‡è°ƒæ•´ä¸»é¢˜æ•°
        n_topics = min(max(len(training_texts) // 10, 5), 30)
        
        print(f"ğŸ“Š è®­ç»ƒæ•°æ®ï¼š{len(training_texts)} ä¸ªæ–‡æœ¬ï¼Œä¸»é¢˜æ•°ï¼š{n_topics}")
        
        # ä½¿ç”¨LDAæ–¹æ³•ï¼ˆæ•ˆæœæœ€ä½³ï¼‰
        self.vectorizer = TopicVectorizer(
            method='lda',
            n_topics=n_topics,
            max_features=min(1000, len(training_texts) * 5),
            max_iter=20
        )
        
        # è®­ç»ƒæ¨¡å‹
        self.vectorizer.train(training_texts)
        
        # ä¿å­˜æ¨¡å‹
        self.vectorizer.save_model(self.model_path)
        self.is_trained = True
        
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶å·²ä¿å­˜")
    
    def load_trained_model(self) -> bool:
        """åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹"""
        try:
            self.vectorizer = TopicVectorizer(method='lda')
            self.vectorizer.load_model(self.model_path)
            self.is_trained = True
            print("âœ… å·²åŠ è½½é¢„è®­ç»ƒæ¨¡å‹")
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def process_all_users(self, users_data: List[Dict[str, Any]]) -> None:
        """æ‰¹é‡å¤„ç†æ‰€æœ‰ç”¨æˆ·"""
        
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train_from_existing_users()æˆ–load_trained_model()")
        
        print("ğŸ”„ æ‰¹é‡å¤„ç†ç”¨æˆ·å‘é‡...")
        
        # åˆ›å»ºåŒ¹é…å™¨
        self.matcher = VectorUserMatcher(self.vectorizer)
        
        # æ‰¹é‡æ·»åŠ ç”¨æˆ·
        self.matcher.add_users(users_data)
        
        # ä¿å­˜ç”¨æˆ·å‘é‡
        self.matcher.save_user_vectors("data/user_vectors/all_users.json")
        
        print(f"âœ… å·²å¤„ç† {len(users_data)} ä¸ªç”¨æˆ·")
    
    def find_matches_for_user(self, user_id: str, top_k: int = 10) -> List[tuple]:
        """ä¸ºæŒ‡å®šç”¨æˆ·æŸ¥æ‰¾åŒ¹é…"""
        if not self.matcher:
            # å°è¯•åŠ è½½ç”¨æˆ·å‘é‡
            self.matcher = VectorUserMatcher(self.vectorizer)
            self.matcher.load_user_vectors("data/user_vectors/all_users.json")
        
        return self.matcher.find_similar_users(user_id, top_k=top_k, min_similarity=0.2)
    
    def add_new_user_and_find_matches(self, new_user: Dict[str, Any], top_k: int = 10) -> List[tuple]:
        """æ·»åŠ æ–°ç”¨æˆ·å¹¶æŸ¥æ‰¾åŒ¹é…"""
        
        if not self.matcher:
            self.matcher = VectorUserMatcher(self.vectorizer)
            self.matcher.load_user_vectors("data/user_vectors/all_users.json")
        
        # æ·»åŠ æ–°ç”¨æˆ·
        self.matcher.add_users([new_user])
        
        # æŸ¥æ‰¾åŒ¹é…
        matches = self.matcher.find_similar_users(
            new_user['user_id'], 
            top_k=top_k, 
            min_similarity=0.2
        )
        
        return matches
    
    def get_similarity_analysis(self, request_type: str = None) -> Dict[str, Any]:
        """è·å–ç›¸ä¼¼åº¦åˆ†ææŠ¥å‘Š"""
        
        if not self.matcher:
            self.matcher = VectorUserMatcher(self.vectorizer)
            self.matcher.load_user_vectors("data/user_vectors/all_users.json")
        
        # è·å–ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix, user_ids = self.matcher.get_similarity_matrix(request_type)
        
        # ç»Ÿè®¡åˆ†æ
        import numpy as np
        
        # æ’é™¤å¯¹è§’çº¿ï¼ˆè‡ªå·±å’Œè‡ªå·±çš„ç›¸ä¼¼åº¦ï¼‰
        mask = ~np.eye(similarity_matrix.shape[0], dtype=bool)
        similarities = similarity_matrix[mask]
        
        analysis = {
            'total_users': len(user_ids),
            'average_similarity': float(np.mean(similarities)),
            'max_similarity': float(np.max(similarities)),
            'min_similarity': float(np.min(similarities)),
            'high_similarity_pairs': int(np.sum(similarities > 0.7)),
            'medium_similarity_pairs': int(np.sum((similarities > 0.3) & (similarities <= 0.7))),
            'low_similarity_pairs': int(np.sum(similarities <= 0.3))
        }
        
        return analysis

def demonstration_with_real_data():
    """ä½¿ç”¨å®é™…æ•°æ®çš„å®Œæ•´æ¼”ç¤º"""
    
    print("ğŸ¯ åŸºäºä¸»é¢˜å‘ç°çš„ç”¨æˆ·åŒ¹é…ç³»ç»Ÿ - å®é™…ä½¿ç”¨æ¼”ç¤º")
    print("=" * 60)
    
    # 1. æ¨¡æ‹Ÿç°æœ‰ç”¨æˆ·æ•°æ®ï¼ˆä»ä½ çš„æ•°æ®åº“è·å–ï¼‰
    existing_users = [
        {
            "user_id": "tech_founder_001",
            "request_type": "æ‰¾é˜Ÿå‹", 
            "text": "èµ„æ·±å…¨æ ˆå·¥ç¨‹å¸ˆï¼Œ10å¹´å¼€å‘ç»éªŒï¼Œæ“…é•¿Pythonã€Reactã€äº‘æ¶æ„ã€‚æƒ³æ‰¾äº§å“åˆä¼™äººä¸€èµ·åšSaaSäº§å“ï¼Œä¸“æ³¨ä¼ä¸šæ•°å­—åŒ–è½¬å‹ã€‚æœ‰è¿‡ä¸¤æ¬¡åˆ›ä¸šç»å†ï¼Œå¸Œæœ›æ‰¾åˆ°æœ‰å•†ä¸šè§†é‡çš„åˆä½œä¼™ä¼´ã€‚"
        },
        {
            "user_id": "product_manager_001",
            "request_type": "æ‰¾é˜Ÿå‹",
            "text": "8å¹´äº§å“ç»éªŒï¼Œæ›¾åœ¨è…¾è®¯ã€å­—èŠ‚æ‹…ä»»é«˜çº§äº§å“ç»ç†ã€‚æ·±åº¦ç†è§£Bç«¯äº§å“å’Œç”¨æˆ·å¢é•¿ï¼Œç†Ÿæ‚‰SaaSå•†ä¸šæ¨¡å¼ã€‚æ­£åœ¨å¯»æ‰¾æŠ€æœ¯åˆä¼™äººï¼Œå¸Œæœ›åšä¸€æ¬¾é¢å‘ä¸­å°ä¼ä¸šçš„æ™ºèƒ½å®¢æœäº§å“ã€‚"
        },
        {
            "user_id": "designer_001", 
            "request_type": "æ‰¾é˜Ÿå‹",
            "text": "UI/UXè®¾è®¡å¸ˆï¼Œ5å¹´äº’è”ç½‘è®¾è®¡ç»éªŒï¼Œæ“…é•¿äº§å“è®¾è®¡å’Œç”¨æˆ·ä½“éªŒã€‚å‚ä¸è¿‡å¤šä¸ª0åˆ°1çš„äº§å“è®¾è®¡ï¼Œå¯¹åˆ›ä¸šæœ‰æµ“åšå…´è¶£ï¼Œå¸Œæœ›åŠ å…¥æ—©æœŸå›¢é˜Ÿè´Ÿè´£äº§å“è®¾è®¡ã€‚"
        },
        {
            "user_id": "ai_researcher_001",
            "request_type": "æ‰¾é˜Ÿå‹", 
            "text": "AIç®—æ³•å·¥ç¨‹å¸ˆï¼Œåšå£«å­¦å†ï¼Œä¸“ç ”æœºå™¨å­¦ä¹ å’ŒNLPã€‚åœ¨é¡¶çº§æœŸåˆŠå‘è¡¨å¤šç¯‡è®ºæ–‡ï¼Œæƒ³å°†AIæŠ€æœ¯å•†ä¸šåŒ–è½åœ°ã€‚å¯»æ‰¾æœ‰å•†ä¸šåŒ–ç»éªŒçš„åˆä¼™äººä¸€èµ·åšAIåº”ç”¨äº§å“ã€‚"
        },
        {
            "user_id": "love_seeker_001",
            "request_type": "æ‰¾å¯¹è±¡",
            "text": "28å²ç¨‹åºå‘˜ï¼Œæ€§æ ¼å†…å‘ä½†å¾ˆæ¸©æš–ï¼Œå¹³æ—¶å–œæ¬¢è¯»ä¹¦ã€å¬éŸ³ä¹ã€çœ‹ç”µå½±ã€‚å·¥ä½œç¨³å®šï¼Œåœ¨ä¸€çº¿äº’è”ç½‘å…¬å¸åšåç«¯å¼€å‘ã€‚å¸Œæœ›æ‰¾ä¸ªå–„è‰¯æ¸©æŸ”ã€æœ‰å…±åŒè¯é¢˜çš„å¥³ç”Ÿï¼Œä¸€èµ·åœ¨åŒ—äº¬ç”Ÿæ´»ã€‚"
        },
        {
            "user_id": "love_seeker_002",
            "request_type": "æ‰¾å¯¹è±¡",
            "text": "26å²è®¾è®¡å¸ˆï¼Œæ€§æ ¼å¼€æœ—å¤–å‘ï¼Œçƒ­çˆ±æ—…è¡Œå’Œæ‘„å½±ï¼Œå‘¨æœ«ç»å¸¸æˆ·å¤–è¿åŠ¨ã€‚å–œæ¬¢å°è¯•æ–°é²œäº‹ç‰©ï¼Œå¯¹ç”Ÿæ´»å……æ»¡çƒ­æƒ…ã€‚å¸Œæœ›æ‰¾ä¸ªé˜³å…‰ç§¯æã€æœ‰ç”Ÿæ´»æƒ…è¶£çš„ç”·ç”Ÿï¼Œä¸€èµ·æ¢ç´¢ä¸–ç•Œã€‚"
        },
        {
            "user_id": "love_seeker_003",
            "request_type": "æ‰¾å¯¹è±¡", 
            "text": "29å²äº§å“ç»ç†ï¼Œå·¥ä½œå’Œç”Ÿæ´»éƒ½å¾ˆæœ‰è§„åˆ’ï¼Œå–œæ¬¢å¥èº«ã€ç¾é£Ÿã€çœ‹ä¹¦ã€‚æ€§æ ¼æˆç†Ÿç¨³é‡ï¼Œæœ‰è‡ªå·±çš„æƒ³æ³•å’Œç›®æ ‡ã€‚å¸Œæœ›æ‰¾ä¸ªåŒæ ·æˆç†Ÿã€æœ‰ä¸Šè¿›å¿ƒçš„äººï¼Œä¸€èµ·è§„åˆ’æœªæ¥ã€‚"
        },
        {
            "user_id": "love_seeker_004",
            "request_type": "æ‰¾å¯¹è±¡",
            "text": "25å²æ–‡æ¡ˆç­–åˆ’ï¼Œçƒ­çˆ±æ–‡å­—å’Œåˆ›æ„ï¼Œå¹³æ—¶å–œæ¬¢å†™ä½œã€çœ‹è¯å‰§ã€é€›å±•è§ˆã€‚æ€§æ ¼æ–‡é™ä½†å†…å¿ƒä¸°å¯Œï¼Œå¯¹ç²¾ç¥ä¸–ç•Œå¾ˆé‡è§†ã€‚å¸Œæœ›æ‰¾ä¸ªæœ‰æ–‡åŒ–åº•è•´ã€èƒ½æ·±åº¦äº¤æµçš„äººã€‚"
        }
    ]
    
    # 2. åˆå§‹åŒ–åŒ¹é…ç³»ç»Ÿ
    matching_system = ProductionMatchingSystem()
    
    # 3. è®­ç»ƒæ¨¡å‹ï¼ˆç¬¬ä¸€æ¬¡è¿è¡Œæ—¶ï¼‰
    print("\næ­¥éª¤1: è®­ç»ƒä¸»é¢˜æ¨¡å‹")
    matching_system.train_from_existing_users(existing_users)
    
    # 4. å¤„ç†ç°æœ‰ç”¨æˆ·
    print("\næ­¥éª¤2: å¤„ç†ç°æœ‰ç”¨æˆ·")
    matching_system.process_all_users(existing_users)
    
    # 5. ä¸ºç°æœ‰ç”¨æˆ·æŸ¥æ‰¾åŒ¹é…
    print("\næ­¥éª¤3: ä¸ºç°æœ‰ç”¨æˆ·æŸ¥æ‰¾åŒ¹é…")
    
    test_users = ["tech_founder_001", "love_seeker_001"]
    
    for user_id in test_users:
        print(f"\nğŸ” ä¸ºç”¨æˆ· {user_id} æŸ¥æ‰¾åŒ¹é…:")
        matches = matching_system.find_matches_for_user(user_id, top_k=3)
        
        if matches:
            for match_user_id, similarity in matches:
                print(f"  ğŸ“ {match_user_id}: ç›¸ä¼¼åº¦ {similarity:.3f} ({similarity*100:.1f}%)")
        else:
            print("  âŒ æœªæ‰¾åˆ°åˆé€‚çš„åŒ¹é…")
    
    # 6. æ¨¡æ‹Ÿæ–°ç”¨æˆ·æ³¨å†Œ
    print("\næ­¥éª¤4: æ–°ç”¨æˆ·æ³¨å†Œå¹¶æŸ¥æ‰¾åŒ¹é…")
    
    new_user = {
        "user_id": "new_tech_partner",
        "request_type": "æ‰¾é˜Ÿå‹",
        "text": "å‰ç«¯å·¥ç¨‹å¸ˆï¼Œ5å¹´Reactç»éªŒï¼Œç†Ÿæ‚‰ç°ä»£å‰ç«¯æŠ€æœ¯æ ˆã€‚æœ‰åˆ›ä¸šæƒ³æ³•ï¼Œå¸Œæœ›åšä¸€ä¸ªé¢å‘ç¨‹åºå‘˜çš„å·¥å…·äº§å“ã€‚å¯»æ‰¾åç«¯æŠ€æœ¯åˆä¼™äººå’Œäº§å“åˆä¼™äººï¼Œä¸€èµ·æ‰“é€ æœ‰ä»·å€¼çš„äº§å“ã€‚"
    }
    
    print(f"æ–°ç”¨æˆ·: {new_user['user_id']}")
    print(f"æè¿°: {new_user['text'][:50]}...")
    
    matches = matching_system.add_new_user_and_find_matches(new_user, top_k=3)
    print("åŒ¹é…ç»“æœ:")
    for match_user_id, similarity in matches:
        print(f"  ğŸ“ {match_user_id}: ç›¸ä¼¼åº¦ {similarity:.3f}")
    
    # 7. ç³»ç»Ÿåˆ†ææŠ¥å‘Š
    print("\næ­¥éª¤5: ç³»ç»Ÿåˆ†ææŠ¥å‘Š")
    
    for request_type in ["æ‰¾é˜Ÿå‹", "æ‰¾å¯¹è±¡"]:
        analysis = matching_system.get_similarity_analysis(request_type)
        print(f"\nğŸ“Š {request_type} ç”¨æˆ·åˆ†æ:")
        print(f"  ç”¨æˆ·æ€»æ•°: {analysis['total_users']}")
        print(f"  å¹³å‡ç›¸ä¼¼åº¦: {analysis['average_similarity']:.3f}")
        print(f"  é«˜ç›¸ä¼¼åº¦é…å¯¹: {analysis['high_similarity_pairs']} å¯¹")
        print(f"  ä¸­ç­‰ç›¸ä¼¼åº¦é…å¯¹: {analysis['medium_similarity_pairs']} å¯¹")
        print(f"  ä½ç›¸ä¼¼åº¦é…å¯¹: {analysis['low_similarity_pairs']} å¯¹")

def production_usage_example():
    """ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ç¤ºä¾‹"""
    
    print("\n" + "="*60)
    print("ğŸ’¼ ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ç¤ºä¾‹")
    print("="*60)
    
    # åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­çš„ä½¿ç”¨æ–¹å¼
    print("""
åœ¨ä½ çš„å®é™…é¡¹ç›®ä¸­ï¼Œå¯ä»¥è¿™æ ·ä½¿ç”¨ï¼š

1. é¦–æ¬¡éƒ¨ç½²ï¼š
```python
# ä»æ•°æ®åº“è·å–ç°æœ‰ç”¨æˆ·
users_from_db = get_all_users_from_database()

# è®­ç»ƒæ¨¡å‹
system = ProductionMatchingSystem()
system.train_from_existing_users(users_from_db)
system.process_all_users(users_from_db)
```

2. æ—¥å¸¸è¿è¡Œï¼š
```python
# å¯åŠ¨æ—¶åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
system = ProductionMatchingSystem()
system.load_trained_model()

# ä¸ºç”¨æˆ·æŸ¥æ‰¾åŒ¹é…
matches = system.find_matches_for_user("user_123", top_k=10)
```

3. æ–°ç”¨æˆ·æ³¨å†Œï¼š
```python
# æ–°ç”¨æˆ·æ³¨å†Œæ—¶
new_user_data = {
    "user_id": request.json['user_id'],
    "request_type": request.json['type'], 
    "text": request.json['description']
}

matches = system.add_new_user_and_find_matches(new_user_data)
```

4. å®šæœŸæ›´æ–°ï¼š
```python
# æ¯å‘¨é‡æ–°è®­ç»ƒæ¨¡å‹ï¼ˆå¯ä»¥è®¾ç½®å®šæ—¶ä»»åŠ¡ï¼‰
def weekly_model_update():
    all_users = get_all_users_from_database()
    system.train_from_existing_users(all_users)
    system.process_all_users(all_users)
```

è¿™æ ·å°±èƒ½åœ¨ä½ çš„ç”Ÿäº§ç¯å¢ƒä¸­å®ç°åŸºäºä¸»é¢˜å‘ç°çš„æ™ºèƒ½åŒ¹é…ï¼
""")

if __name__ == "__main__":
    demonstration_with_real_data()
    production_usage_example() 