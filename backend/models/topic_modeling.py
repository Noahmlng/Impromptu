#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import numpy as np
import jieba
import re
from typing import Dict, List, Tuple, Set, Any
from dataclasses import dataclass
from sklearn.feature_extraction.text import TfidfVectorizer
from gensim import corpora, models
from gensim.parsing.preprocessing import STOPWORDS
from configs.config import TopicModelingConfig
from .tag_pool import TagPool, TagCategory

@dataclass
class TopicResult:
    """ä¸»é¢˜å»ºæ¨¡ç»“æœ"""
    topics: List[Tuple[int, float]]  # [(topic_id, weight), ...]
    extracted_tags: Dict[str, float]  # {tag: confidence, ...}
    topic_keywords: Dict[int, List[Tuple[str, float]]]  # {topic_id: [(word, weight), ...]}
    text_vector: List[float]  # æ–‡æœ¬å‘é‡è¡¨ç¤º

class ChineseTextPreprocessor:
    """ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†å™¨"""
    
    def __init__(self):
        # ä¸­æ–‡åœç”¨è¯ï¼ˆå‡å°‘åœç”¨è¯æ•°é‡ï¼Œé¿å…è¿‡åº¦è¿‡æ»¤ï¼‰
        self.stopwords = set([
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
            'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
            'è‡ªå·±', 'è¿™', 'é‚£', 'ç°åœ¨', 'å¯ä»¥', 'ä½†æ˜¯', 'å¦‚æœ',
            'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘ä»¬', 'ä»–ä»¬', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ'
        ])
        
        # åªä¿ç•™å¸¸è§çš„è‹±æ–‡åœç”¨è¯
        common_english_stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        self.stopwords.update(common_english_stopwords)
    
    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        if not text:
            return ""
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’Œç©ºæ ¼
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s+\-/]', ' ', text)
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def tokenize(self, text: str) -> List[str]:
        """åˆ†è¯"""
        text = self.clean_text(text)
        if not text:
            return []
        
        print(f"åˆ†è¯å‰æ–‡æœ¬: {text[:100]}...")
        
        # ä¸­æ–‡åˆ†è¯
        tokens = []
        for word in jieba.cut(text):
            word = word.strip()
            if (len(word) >= 1 and  # é•¿åº¦å¤§äºç­‰äº1ï¼ˆæ”¾å®½æ¡ä»¶ï¼‰
                word not in self.stopwords and  # ä¸æ˜¯åœç”¨è¯
                not word.isdigit() and  # ä¸æ˜¯çº¯æ•°å­—
                word.isalpha() or len(word) > 1):  # æ˜¯å­—æ¯æˆ–é•¿åº¦å¤§äº1
                tokens.append(word)
        
        print(f"åˆ†è¯ç»“æœ: {tokens[:20]}...")
        return tokens
    
    def preprocess_documents(self, texts: List[str]) -> List[List[str]]:
        """é¢„å¤„ç†æ–‡æ¡£é›†åˆ"""
        processed = []
        for i, text in enumerate(texts):
            tokens = self.tokenize(text)
            print(f"æ–‡æ¡£ {i+1} åˆ†è¯å¾—åˆ° {len(tokens)} ä¸ªè¯æ±‡")
            processed.append(tokens)
        return processed

class LDATopicModel:
    """LDAä¸»é¢˜å»ºæ¨¡å™¨"""
    
    def __init__(self, config: TopicModelingConfig = None):
        self.config = config or TopicModelingConfig()
        # é™ä½ä¸»é¢˜æ•°é‡ï¼Œå› ä¸ºæˆ‘ä»¬åªæœ‰2ä¸ªæ–‡æ¡£
        self.config.num_topics = min(self.config.num_topics, 5)
        self.preprocessor = ChineseTextPreprocessor()
        
        self.dictionary = None
        self.corpus = None
        self.lda_model = None
        self.tag_pool = TagPool()
        
        # æ ‡ç­¾åˆ°ä¸»é¢˜çš„æ˜ å°„
        self.tag_topic_mapping = {}
        
    def train(self, documents: List[str]) -> None:
        """è®­ç»ƒLDAæ¨¡å‹"""
        print(f"å¼€å§‹è®­ç»ƒLDAæ¨¡å‹ï¼Œæ–‡æ¡£æ•°é‡: {len(documents)}")
        
        # é¢„å¤„ç†æ–‡æ¡£
        processed_docs = self.preprocessor.preprocess_documents(documents)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„æ–‡æ¡£
        valid_docs = [doc for doc in processed_docs if len(doc) > 0]
        if not valid_docs:
            print("è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æ¡£å¯ä»¥è®­ç»ƒLDAæ¨¡å‹")
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„å‡æ•°æ®æ¥é¿å…é”™è¯¯
            valid_docs = [['åˆ›ä¸š', 'AI', 'æŠ€æœ¯'], ['åˆä½œ', 'äº§å“', 'å¼€å‘']]
        
        print(f"æœ‰æ•ˆæ–‡æ¡£æ•°é‡: {len(valid_docs)}")
        
        # åˆ›å»ºè¯å…¸
        self.dictionary = corpora.Dictionary(valid_docs)
        
        # æ”¾å®½è¿‡æ»¤æ¡ä»¶
        self.dictionary.filter_extremes(
            no_below=1,  # é™ä½åˆ°1
            no_above=0.9,  # æé«˜åˆ°0.9
            keep_n=2000   # å¢åŠ åˆ°2000
        )
        
        print(f"è¿‡æ»¤åè¯å…¸å¤§å°: {len(self.dictionary)}")
        
        if len(self.dictionary) == 0:
            print("è¯å…¸ä¸ºç©ºï¼Œåˆ›å»ºæœ€å°è¯å…¸")
            # æ‰‹åŠ¨åˆ›å»ºä¸€äº›åŸºæœ¬è¯æ±‡
            basic_words = [['AI', 'äººå·¥æ™ºèƒ½', 'åˆ›ä¸š', 'æŠ€æœ¯', 'äº§å“', 'åˆä½œ', 'å¼€å‘']]
            self.dictionary = corpora.Dictionary(basic_words)
        
        # åˆ›å»ºè¯­æ–™åº“
        self.corpus = [self.dictionary.doc2bow(doc) for doc in valid_docs]
        
        print(f"è¯­æ–™åº“å¤§å°: {len(self.corpus)}")
        print(f"è¯å…¸æœ€ç»ˆå¤§å°: {len(self.dictionary)}")
        
        # è°ƒæ•´LDAå‚æ•°ä»¥é€‚åº”å°æ•°æ®é›†
        num_topics = min(self.config.num_topics, len(self.dictionary), 3)
        
        # è®­ç»ƒLDAæ¨¡å‹
        self.lda_model = models.LdaModel(
            corpus=self.corpus,
            id2word=self.dictionary,
            num_topics=num_topics,
            random_state=42,
            passes=5,  # å‡å°‘è®­ç»ƒè½®æ•°
            iterations=20,  # å‡å°‘è¿­ä»£æ¬¡æ•°
            alpha='auto',
            eta='auto'
        )
        
        print(f"LDAæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œä¸»é¢˜æ•°é‡: {num_topics}")
        
        # å»ºç«‹æ ‡ç­¾åˆ°ä¸»é¢˜çš„æ˜ å°„
        self._build_tag_topic_mapping()
    
    def _build_tag_topic_mapping(self):
        """å»ºç«‹æ ‡ç­¾åˆ°ä¸»é¢˜çš„æ˜ å°„å…³ç³»"""
        if not self.lda_model:
            return
        
        print("å»ºç«‹æ ‡ç­¾åˆ°ä¸»é¢˜çš„æ˜ å°„å…³ç³»...")
        
        # è·å–æ‰€æœ‰æ ‡ç­¾
        all_tags = self.tag_pool.get_tag_list()
        
        # ä¸ºæ¯ä¸ªæ ‡ç­¾æ‰¾åˆ°æœ€ç›¸å…³çš„ä¸»é¢˜
        mapped_count = 0
        for tag in all_tags:
            # å°†æ ‡ç­¾ä½œä¸ºæ–‡æ¡£è¿›è¡Œé¢„å¤„ç†
            tag_tokens = self.preprocessor.tokenize(tag)
            if not tag_tokens:
                continue
                
            # è½¬æ¢ä¸ºBOW
            tag_bow = self.dictionary.doc2bow(tag_tokens)
            if not tag_bow:
                continue
            
            # è·å–ä¸»é¢˜åˆ†å¸ƒ
            topic_dist = self.lda_model.get_document_topics(tag_bow, minimum_probability=0)
            
            # æ‰¾åˆ°æœ€ç›¸å…³çš„ä¸»é¢˜
            if topic_dist:
                best_topics = sorted(topic_dist, key=lambda x: x[1], reverse=True)[:3]
                self.tag_topic_mapping[tag] = best_topics
                mapped_count += 1
        
        print(f"å®Œæˆæ ‡ç­¾æ˜ å°„ï¼Œæ˜ å°„äº† {mapped_count} ä¸ªæ ‡ç­¾")
    
    def extract_topics_and_tags(self, text: str, request_type: str = "all") -> TopicResult:
        """ä»æ–‡æœ¬ä¸­æå–ä¸»é¢˜å’Œæ ‡ç­¾"""
        print(f"ğŸ” [TopicModel] å¼€å§‹æå–æ ‡ç­¾ï¼Œè¯·æ±‚ç±»å‹: {request_type}")
        print(f"ğŸ“ [TopicModel] è¾“å…¥æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        
        # å¦‚æœæ¨¡å‹æœªè®­ç»ƒï¼Œç›´æ¥ä½¿ç”¨å…³é”®è¯åŒ¹é…
        if not self.lda_model:
            print("âš ï¸ [TopicModel] LDAæ¨¡å‹æœªè®­ç»ƒï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…æ–¹å¼")
            extracted_tags = self._extract_tags_by_keywords(text, request_type)
            return TopicResult(
                topics=[],
                extracted_tags=extracted_tags,
                topic_keywords={},
                text_vector=[]
            )
        
        # é¢„å¤„ç†æ–‡æœ¬
        tokens = self.preprocessor.tokenize(text)
        print(f"ğŸ”¤ [TopicModel] åˆ†è¯ç»“æœ: {len(tokens)} ä¸ªè¯æ±‡")
        
        if not tokens:
            print("âš ï¸ [TopicModel] åˆ†è¯ç»“æœä¸ºç©ºï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…")
            extracted_tags = self._extract_tags_by_keywords(text, request_type)
            return TopicResult(
                topics=[],
                extracted_tags=extracted_tags,
                topic_keywords={},
                text_vector=[0.0] * self.lda_model.num_topics
            )
        
        # è½¬æ¢ä¸ºBOW
        bow = self.dictionary.doc2bow(tokens)
        print(f"ğŸ“Š [TopicModel] BOWå‘é‡é•¿åº¦: {len(bow)}")
        
        if not bow:
            print("âš ï¸ [TopicModel] BOWå‘é‡ä¸ºç©ºï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…")
            extracted_tags = self._extract_tags_by_keywords(text, request_type)
            return TopicResult(
                topics=[],
                extracted_tags=extracted_tags,
                topic_keywords={},
                text_vector=[0.0] * self.lda_model.num_topics
            )
        
        # è·å–ä¸»é¢˜åˆ†å¸ƒ
        topic_distribution = self.lda_model.get_document_topics(
            bow, 
            minimum_probability=0.01  # é™ä½é˜ˆå€¼
        )
        print(f"ğŸ“ˆ [TopicModel] ä¸»é¢˜åˆ†å¸ƒ: {len(topic_distribution)} ä¸ªä¸»é¢˜")
        
        # è·å–ä¸»é¢˜å…³é”®è¯
        topic_keywords = {}
        for topic_id, _ in topic_distribution:
            words = self.lda_model.show_topic(topic_id, topn=10)
            topic_keywords[topic_id] = words
        
        # åŸºäºä¸»é¢˜åˆ†å¸ƒæå–æ ‡ç­¾
        extracted_tags = self._extract_tags_from_topics(
            topic_distribution, 
            request_type
        )
        print(f"ğŸ·ï¸ [TopicModel] ä»ä¸»é¢˜æå–åˆ° {len(extracted_tags)} ä¸ªæ ‡ç­¾")
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°æ ‡ç­¾ï¼Œä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…
        if not extracted_tags:
            print("ğŸ”„ [TopicModel] ä¸»é¢˜æå–ä¸ºç©ºï¼Œæ”¹ç”¨å…³é”®è¯åŒ¹é…")
            extracted_tags = self._extract_tags_by_keywords(text, request_type)
            print(f"ğŸ·ï¸ [TopicModel] å…³é”®è¯åŒ¹é…æå–åˆ° {len(extracted_tags)} ä¸ªæ ‡ç­¾")
        
        # ç”Ÿæˆæ–‡æœ¬å‘é‡ï¼ˆä¸»é¢˜æ¦‚ç‡åˆ†å¸ƒï¼‰
        text_vector = [0.0] * self.lda_model.num_topics
        for topic_id, prob in topic_distribution:
            text_vector[topic_id] = prob
        
        print(f"âœ… [TopicModel] æ ‡ç­¾æå–å®Œæˆï¼Œæ€»å…± {len(extracted_tags)} ä¸ªæ ‡ç­¾")
        return TopicResult(
            topics=topic_distribution,
            extracted_tags=extracted_tags,
            topic_keywords=topic_keywords,
            text_vector=text_vector
        )
    
    def _extract_tags_by_keywords(self, text: str, request_type: str) -> Dict[str, float]:
        """åŸºäºå…³é”®è¯åŒ¹é…æå–æ ‡ç­¾"""
        print(f"ğŸ” [TopicModel] å¼€å§‹å…³é”®è¯åŒ¹é…ï¼Œè¯·æ±‚ç±»å‹: {request_type}")
        text_lower = text.lower()
        extracted_tags = {}
        
        # è·å–ç›¸å…³æ ‡ç­¾æ± 
        try:
            relevant_tags = self.tag_pool.get_tag_list(request_type)
            print(f"ğŸ“‹ [TopicModel] è·å–åˆ° {len(relevant_tags)} ä¸ªå€™é€‰æ ‡ç­¾")
        except Exception as e:
            print(f"âš ï¸ [TopicModel] è·å–æ ‡ç­¾æ± å¤±è´¥: {e}")
            relevant_tags = []
        
        # æ ‡ç­¾åŒ¹é…
        matched_count = 0
        for tag in relevant_tags:
            tag_lower = tag.lower()
            # å®Œæ•´åŒ¹é…
            if tag_lower in text_lower:
                extracted_tags[tag] = 0.8
                matched_count += 1
            # éƒ¨åˆ†è¯åŒ¹é…
            elif any(word in text_lower for word in tag_lower.split() if len(word) > 1):
                extracted_tags[tag] = 0.6
                matched_count += 1
        
        print(f"ğŸ¯ [TopicModel] ä»æ ‡ç­¾æ± åŒ¹é…åˆ° {matched_count} ä¸ªæ ‡ç­¾")
        
        # åŸºäºå†…å®¹çš„æ ‡ç­¾è§„åˆ™ï¼ˆå¢å¼ºç‰ˆï¼‰
        content_rules = {
            # æŠ€æœ¯ç›¸å…³
            ('ai', 'äººå·¥æ™ºèƒ½', 'machine learning', 'æœºå™¨å­¦ä¹ ', 'deep learning', 'æ·±åº¦å­¦ä¹ '): 'äººå·¥æ™ºèƒ½',
            ('åˆ›ä¸š', 'startup', 'åˆ›æ–°', 'ä¼ä¸šå®¶'): 'åˆ›ä¸šè€…',
            ('æŠ€æœ¯', 'technology', 'å¼€å‘', 'development', 'ç¼–ç¨‹', 'programming'): 'æŠ€æœ¯å‹',
            ('äº§å“', 'product', 'äº§å“ç»ç†', 'pm'): 'äº§å“ç®¡ç†',
            ('è®¾è®¡', 'design', 'ui', 'ux', 'ç”¨æˆ·ä½“éªŒ'): 'è®¾è®¡å¸ˆ',
            ('æ•°æ®', 'data', 'åˆ†æ', 'analytics', 'æ•°æ®ç§‘å­¦'): 'æ•°æ®åˆ†æ',
            ('ç ”ç©¶', 'research', 'ç§‘ç ”', 'å­¦æœ¯'): 'ç ”ç©¶å‹',
            ('ç®¡ç†', 'management', 'é¢†å¯¼', 'leader', 'å›¢é˜Ÿ'): 'ç®¡ç†å‹',
            ('è¥é”€', 'marketing', 'å¸‚åœº', 'æ¨å¹¿'): 'å¸‚åœºè¥é”€',
            ('è¿è¥', 'operation', 'è¿è¥ç®¡ç†'): 'è¿è¥ä¸“å®¶',
            
            # æ€§æ ¼ç‰¹å¾
            ('å¤–å‘', 'å¼€æœ—', 'æ´»æ³¼', 'ç¤¾äº¤'): 'å¤–å‘å‹',
            ('å†…å‘', 'å®‰é™', 'æ€è€ƒ', 'ç‹¬ç«‹'): 'å†…å‘å‹',
            ('å¹½é»˜', 'æœ‰è¶£', 'æç¬‘', 'é£è¶£'): 'å¹½é»˜æ„Ÿ',
            ('è®¤çœŸ', 'è´Ÿè´£', 'å¯é ', 'è´£ä»»å¿ƒ'): 'è´£ä»»æ„Ÿ',
            ('åˆ›æ„', 'åˆ›é€ ', 'æƒ³è±¡', 'è‰ºæœ¯'): 'åˆ›æ„å‹',
            ('é€»è¾‘', 'ç†æ€§', 'åˆ†æ', 'æ€ç»´'): 'é€»è¾‘æ€ç»´',
            
            # å…´è¶£çˆ±å¥½
            ('æ—…è¡Œ', 'æ—…æ¸¸', 'æ¢ç´¢', 'å†’é™©'): 'æ—…è¡Œçˆ±å¥½è€…',
            ('è¿åŠ¨', 'å¥èº«', 'é”»ç‚¼', 'fitness'): 'è¿åŠ¨è¾¾äºº',
            ('è¯»ä¹¦', 'é˜…è¯»', 'å­¦ä¹ ', 'çŸ¥è¯†'): 'å­¦ä¹ å‹',
            ('éŸ³ä¹', 'æ­Œæ›²', 'ä¹å™¨', 'æ¼”å¥'): 'éŸ³ä¹çˆ±å¥½è€…',
            ('æ‘„å½±', 'æ‹ç…§', 'ç›¸æœº', 'é•œå¤´'): 'æ‘„å½±çˆ±å¥½è€…',
            ('æ¸¸æˆ', 'ç”µç«', 'gaming'): 'æ¸¸æˆçˆ±å¥½è€…',
            ('ç¾é£Ÿ', 'çƒ¹é¥ª', 'æ–™ç†', 'å¨è‰º'): 'ç¾é£Ÿå®¶',
        }
        
        rule_matched_count = 0
        for keywords, tag_name in content_rules.items():
            for keyword in keywords:
                if keyword in text_lower:
                    extracted_tags[tag_name] = min(extracted_tags.get(tag_name, 0) + 0.2, 0.9)
                    rule_matched_count += 1
                    break
        
        print(f"ğŸ“ [TopicModel] ä»å†…å®¹è§„åˆ™åŒ¹é…åˆ° {rule_matched_count} ä¸ªæ ‡ç­¾")
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ ‡ç­¾ï¼Œæ·»åŠ ä¸€äº›é€šç”¨æ ‡ç­¾
        if not extracted_tags:
            print("ğŸ”„ [TopicModel] æœªåŒ¹é…åˆ°ä»»ä½•æ ‡ç­¾ï¼Œæ·»åŠ é€šç”¨æ ‡ç­¾")
            if request_type == 'æ‰¾å¯¹è±¡':
                extracted_tags.update({
                    'å¯»æ‰¾ä¼´ä¾£': 0.6,
                    'çœŸè¯šäº¤å‹': 0.5,
                    'é•¿æœŸå…³ç³»': 0.5
                })
            elif request_type == 'æ‰¾é˜Ÿå‹':
                extracted_tags.update({
                    'å¯»æ‰¾åˆä½œ': 0.6,
                    'å›¢é˜Ÿåä½œ': 0.5,
                    'å…±åŒæˆé•¿': 0.5
                })
            else:
                extracted_tags.update({
                    'çœŸè¯š': 0.5,
                    'å‹å–„': 0.5,
                    'ç§¯æ': 0.5
                })
        
        print(f"âœ… [TopicModel] å…³é”®è¯åŒ¹é…å®Œæˆï¼Œå…±æå– {len(extracted_tags)} ä¸ªæ ‡ç­¾")
        return extracted_tags
    
    def _extract_tags_from_topics(self, topic_distribution: List[Tuple[int, float]], 
                                 request_type: str) -> Dict[str, float]:
        """åŸºäºä¸»é¢˜åˆ†å¸ƒæå–æ ‡ç­¾"""
        extracted_tags = {}
        
        # è·å–ç›¸å…³æ ‡ç­¾æ± 
        relevant_tags = self.tag_pool.get_tag_list(request_type)
        
        for tag in relevant_tags:
            if tag not in self.tag_topic_mapping:
                continue
            
            # è®¡ç®—æ ‡ç­¾çš„ç½®ä¿¡åº¦
            confidence = 0.0
            for tag_topic_id, tag_topic_weight in self.tag_topic_mapping[tag]:
                # æ‰¾åˆ°æ–‡æ¡£ä¸­å¯¹åº”ä¸»é¢˜çš„æƒé‡
                doc_topic_weight = 0.0
                for doc_topic_id, doc_topic_prob in topic_distribution:
                    if doc_topic_id == tag_topic_id:
                        doc_topic_weight = doc_topic_prob
                        break
                
                confidence += tag_topic_weight * doc_topic_weight
            
            # é™ä½ç½®ä¿¡åº¦é˜ˆå€¼
            if confidence >= 0.1:
                extracted_tags[tag] = confidence
        
        return extracted_tags
    
    def get_topic_info(self) -> Dict[int, Dict[str, Any]]:
        """è·å–ä¸»é¢˜ä¿¡æ¯"""
        if not self.lda_model:
            return {}
        
        topic_info = {}
        for topic_id in range(self.lda_model.num_topics):
            words = self.lda_model.show_topic(topic_id, topn=10)
            topic_info[topic_id] = {
                'keywords': words,
                'description': ', '.join([word for word, _ in words[:5]])
            }
        
        return topic_info
    
    def save_model(self, model_path: str) -> None:
        """ä¿å­˜æ¨¡å‹"""
        if self.lda_model:
            self.lda_model.save(f"{model_path}_lda")
            self.dictionary.save(f"{model_path}_dict")
            
            # ä¿å­˜æ ‡ç­¾æ˜ å°„ï¼Œå¤„ç†float32ç±»å‹
            serializable_mapping = {}
            for tag, topics in self.tag_topic_mapping.items():
                serializable_mapping[tag] = [(int(tid), float(weight)) for tid, weight in topics]
            
            with open(f"{model_path}_tag_mapping.json", 'w', encoding='utf-8') as f:
                json.dump(serializable_mapping, f, ensure_ascii=False, indent=2)
            
            print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    
    def load_model(self, model_path: str) -> None:
        """åŠ è½½æ¨¡å‹"""
        try:
            self.lda_model = models.LdaModel.load(f"{model_path}_lda")
            self.dictionary = corpora.Dictionary.load(f"{model_path}_dict")
            
            # åŠ è½½æ ‡ç­¾æ˜ å°„
            with open(f"{model_path}_tag_mapping.json", 'r', encoding='utf-8') as f:
                self.tag_topic_mapping = json.load(f)
            
            print(f"æ¨¡å‹å·²ä» {model_path} åŠ è½½")
        except Exception as e:
            print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            raise

# å…¨å±€å®ä¾‹
topic_model = LDATopicModel()

# è‡ªåŠ¨åŠ è½½ç”Ÿäº§æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    import os
    production_model_path = "data/models/production_model"
    if (os.path.exists(f"{production_model_path}_lda") and 
        os.path.exists(f"{production_model_path}_dict") and 
        os.path.exists(f"{production_model_path}_tag_mapping.json")):
        topic_model.load_model(production_model_path)
        print("å·²è‡ªåŠ¨åŠ è½½ç”Ÿäº§LDAæ¨¡å‹")
except Exception as e:
    print(f"åŠ è½½ç”Ÿäº§æ¨¡å‹å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®: {e}") 