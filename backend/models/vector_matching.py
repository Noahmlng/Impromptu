#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŸºäºä¸»é¢˜å‘ç°å’Œå‘é‡ç›¸ä¼¼åº¦çš„ç”¨æˆ·åŒ¹é…ç³»ç»Ÿ

å·¥ä½œæµç¨‹:
1. æ–‡æœ¬é¢„å¤„ç†å’Œç‰¹å¾æå–
2. è®­ç»ƒä¸»é¢˜æ¨¡å‹ï¼ˆLDA/Doc2Vec/TF-IDFï¼‰
3. å°†ç”¨æˆ·æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
4. è®¡ç®—ç”¨æˆ·é—´çš„cosineç›¸ä¼¼åº¦
5. åŸºäºç›¸ä¼¼åº¦è¿›è¡ŒåŒ¹é…æ¨è
"""

import json
import numpy as np
import jieba
import re
import pickle
import os
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import LatentDirichletAllocation
from gensim import corpora, models
from gensim.models import Doc2Vec
from gensim.models.doc2vec import TaggedDocument
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class UserVector:
    """ç”¨æˆ·å‘é‡è¡¨ç¤º"""
    user_id: str
    request_type: str
    text: str
    vector: np.ndarray
    topics: Optional[List[Tuple[int, float]]] = None
    tags: Optional[List[str]] = None
    metadata: Optional[Dict[str, Any]] = None
    
class ChineseTextProcessor:
    """ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†å™¨"""
    
    def __init__(self):
        self.stopwords = set([
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
            'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
            'è‡ªå·±', 'è¿™', 'é‚£', 'ç°åœ¨', 'å¯ä»¥', 'ä½†æ˜¯', 'å¦‚æœ', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘ä»¬',
            'ä»–ä»¬', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'é‚£ä¸ª', 'è¿™ä¸ª', 'è¿˜æ˜¯', 'æˆ–è€…', 'å› ä¸º',
            'æ‰€ä»¥', 'è™½ç„¶', 'ä½†æ˜¯', 'ç„¶å', 'å¼€å§‹', 'ç»“æŸ', 'æ—¶å€™', 'åœ°æ–¹', 'é—®é¢˜',
            'æ–¹æ³•', 'å¯èƒ½', 'åº”è¯¥', 'éœ€è¦', 'å¸Œæœ›', 'è§‰å¾—', 'è®¤ä¸º', 'çŸ¥é“', 'çœ‹åˆ°'
        ])
    
    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        if not text:
            return ""
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', ' ', text)
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def tokenize(self, text: str) -> List[str]:
        """åˆ†è¯å¹¶è¿‡æ»¤åœç”¨è¯"""
        text = self.clean_text(text)
        if not text:
            return []
        
        tokens = []
        for word in jieba.cut(text):
            word = word.strip()
            if (len(word) >= 2 and 
                word not in self.stopwords and 
                not word.isdigit() and
                not word.isspace()):
                tokens.append(word)
        
        return tokens
    
    def preprocess_documents(self, texts: List[str]) -> List[List[str]]:
        """æ‰¹é‡é¢„å¤„ç†æ–‡æ¡£"""
        return [self.tokenize(text) for text in texts]

class TopicVectorizer:
    """ä¸»é¢˜å‘é‡åŒ–å™¨ - æ”¯æŒå¤šç§å‘é‡åŒ–æ–¹æ³•"""
    
    def __init__(self, method='tfidf', **kwargs):
        """
        åˆå§‹åŒ–å‘é‡åŒ–å™¨
        
        Args:
            method: å‘é‡åŒ–æ–¹æ³• ('tfidf', 'lda', 'doc2vec')
            **kwargs: å„æ–¹æ³•çš„å‚æ•°
        """
        self.method = method
        self.text_processor = ChineseTextProcessor()
        self.is_trained = False
        
        # åˆå§‹åŒ–æ¨¡å‹
        if method == 'tfidf':
            self.model = TfidfVectorizer(
                tokenizer=self.text_processor.tokenize,
                lowercase=False,
                max_features=kwargs.get('max_features', 1000),
                min_df=kwargs.get('min_df', 2),
                max_df=kwargs.get('max_df', 0.8)
            )
        elif method == 'lda':
            self.n_topics = kwargs.get('n_topics', 20)
            self.lda_model = LatentDirichletAllocation(
                n_components=self.n_topics,
                random_state=42,
                max_iter=kwargs.get('max_iter', 10)
            )
            self.vectorizer = TfidfVectorizer(
                tokenizer=self.text_processor.tokenize,
                lowercase=False,
                max_features=kwargs.get('max_features', 1000)
            )
        elif method == 'doc2vec':
            self.vector_size = kwargs.get('vector_size', 100)
            self.model = Doc2Vec(
                vector_size=self.vector_size,
                window=kwargs.get('window', 5),
                min_count=kwargs.get('min_count', 2),
                workers=kwargs.get('workers', 4),
                epochs=kwargs.get('epochs', 10)
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å‘é‡åŒ–æ–¹æ³•: {method}")
    
    def train(self, texts: List[str]) -> None:
        """è®­ç»ƒå‘é‡åŒ–æ¨¡å‹"""
        logger.info(f"å¼€å§‹è®­ç»ƒ{self.method}æ¨¡å‹ï¼Œæ–‡æ¡£æ•°é‡: {len(texts)}")
        
        if self.method == 'tfidf':
            self.model.fit(texts)
            
        elif self.method == 'lda':
            # å…ˆç”¨TF-IDFå¤„ç†
            tfidf_matrix = self.vectorizer.fit_transform(texts)
            # è®­ç»ƒLDA
            self.lda_model.fit(tfidf_matrix)
            
        elif self.method == 'doc2vec':
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            processed_docs = self.text_processor.preprocess_documents(texts)
            tagged_docs = [
                TaggedDocument(words=doc, tags=[f'doc_{i}']) 
                for i, doc in enumerate(processed_docs)
            ]
            
            # æ„å»ºè¯æ±‡è¡¨
            self.model.build_vocab(tagged_docs)
            # è®­ç»ƒæ¨¡å‹
            self.model.train(tagged_docs, total_examples=self.model.corpus_count, epochs=self.model.epochs)
        
        self.is_trained = True
        logger.info(f"{self.method}æ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    def transform(self, texts: List[str]) -> np.ndarray:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train()æ–¹æ³•")
        
        if self.method == 'tfidf':
            return self.model.transform(texts).toarray()
            
        elif self.method == 'lda':
            tfidf_matrix = self.vectorizer.transform(texts)
            return self.lda_model.transform(tfidf_matrix)
            
        elif self.method == 'doc2vec':
            vectors = []
            for text in texts:
                tokens = self.text_processor.tokenize(text)
                if tokens:
                    vector = self.model.infer_vector(tokens)
                    vectors.append(vector)
                else:
                    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆtokensï¼Œè¿”å›é›¶å‘é‡
                    vectors.append(np.zeros(self.vector_size))
            return np.array(vectors)
    
    def save_model(self, model_path: str) -> None:
        """ä¿å­˜æ¨¡å‹"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        model_data = {
            'method': self.method,
            'is_trained': self.is_trained
        }
        
        if self.method == 'tfidf':
            with open(f"{model_path}_tfidf.pkl", 'wb') as f:
                pickle.dump(self.model, f)
                
        elif self.method == 'lda':
            with open(f"{model_path}_vectorizer.pkl", 'wb') as f:
                pickle.dump(self.vectorizer, f)
            with open(f"{model_path}_lda.pkl", 'wb') as f:
                pickle.dump(self.lda_model, f)
            model_data['n_topics'] = self.n_topics
            
        elif self.method == 'doc2vec':
            self.model.save(f"{model_path}_doc2vec.model")
            model_data['vector_size'] = self.vector_size
        
        # ä¿å­˜å…ƒæ•°æ®
        with open(f"{model_path}_metadata.json", 'w', encoding='utf-8') as f:
            json.dump(model_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    
    def load_model(self, model_path: str) -> None:
        """åŠ è½½æ¨¡å‹"""
        # åŠ è½½å…ƒæ•°æ®
        with open(f"{model_path}_metadata.json", 'r', encoding='utf-8') as f:
            model_data = json.load(f)
        
        self.method = model_data['method']
        self.is_trained = model_data['is_trained']
        
        if self.method == 'tfidf':
            with open(f"{model_path}_tfidf.pkl", 'rb') as f:
                self.model = pickle.load(f)
                
        elif self.method == 'lda':
            with open(f"{model_path}_vectorizer.pkl", 'rb') as f:
                self.vectorizer = pickle.load(f)
            with open(f"{model_path}_lda.pkl", 'rb') as f:
                self.lda_model = pickle.load(f)
            self.n_topics = model_data['n_topics']
            
        elif self.method == 'doc2vec':
            self.model = Doc2Vec.load(f"{model_path}_doc2vec.model")
            self.vector_size = model_data['vector_size']
        
        logger.info(f"æ¨¡å‹å·²ä» {model_path} åŠ è½½")

class VectorUserMatcher:
    """åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„ç”¨æˆ·åŒ¹é…å™¨"""
    
    def __init__(self, vectorizer: TopicVectorizer):
        self.vectorizer = vectorizer
        self.user_vectors: Dict[str, UserVector] = {}
        self.users = self.user_vectors  # ä¸ºäº†å…¼å®¹æ€§æ·»åŠ çš„åˆ«å
        self.vectors_matrix: Optional[np.ndarray] = None
        self.user_ids: List[str] = []
    
    def add_users(self, users_data: List[Dict[str, Any]]) -> None:
        """æ·»åŠ ç”¨æˆ·å¹¶è®¡ç®—å‘é‡"""
        texts = [user['text'] for user in users_data]
        
        # æ‰¹é‡è½¬æ¢ä¸ºå‘é‡
        vectors = self.vectorizer.transform(texts)
        
        # å­˜å‚¨ç”¨æˆ·å‘é‡
        for i, user in enumerate(users_data):
            user_vector = UserVector(
                user_id=user['user_id'],
                request_type=user['request_type'],
                text=user['text'],
                vector=vectors[i]
            )
            self.user_vectors[user['user_id']] = user_vector
        
        # æ›´æ–°å‘é‡çŸ©é˜µ
        self._update_vectors_matrix()
        
        logger.info(f"å·²æ·»åŠ  {len(users_data)} ä¸ªç”¨æˆ·")
    
    def _update_vectors_matrix(self) -> None:
        """æ›´æ–°å‘é‡çŸ©é˜µç”¨äºæ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦"""
        if not self.user_vectors:
            return
        
        self.user_ids = list(self.user_vectors.keys())
        self.vectors_matrix = np.vstack([
            self.user_vectors[user_id].vector 
            for user_id in self.user_ids
        ])
    
    def find_similar_users(self, target_user_id: str, 
                          request_type: str = None,
                          top_k: int = 10,
                          min_similarity: float = 0.1) -> List[Tuple[str, float]]:
        """ä¸ºç›®æ ‡ç”¨æˆ·æ‰¾åˆ°ç›¸ä¼¼çš„ç”¨æˆ·"""
        
        if target_user_id not in self.user_vectors:
            raise ValueError(f"ç”¨æˆ· {target_user_id} ä¸å­˜åœ¨")
        
        target_vector = self.user_vectors[target_user_id]
        target_request_type = target_vector.request_type
        
        # è¿‡æ»¤ç›¸åŒè¯·æ±‚ç±»å‹çš„ç”¨æˆ·
        candidate_indices = []
        candidate_ids = []
        
        for i, user_id in enumerate(self.user_ids):
            if (user_id != target_user_id and 
                self.user_vectors[user_id].request_type == target_request_type):
                candidate_indices.append(i)
                candidate_ids.append(user_id)
        
        if not candidate_indices:
            return []
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        target_vec = target_vector.vector.reshape(1, -1)
        candidate_vectors = self.vectors_matrix[candidate_indices]
        
        similarities = cosine_similarity(target_vec, candidate_vectors).flatten()
        
        # ç­›é€‰å’Œæ’åº
        results = []
        for i, sim in enumerate(similarities):
            if sim >= min_similarity:
                results.append((candidate_ids[i], float(sim)))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x[1], reverse=True)
        
        return results[:top_k]
    
    def get_similarity_matrix(self, request_type: str = None) -> Tuple[np.ndarray, List[str]]:
        """è·å–ç”¨æˆ·é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ"""
        # ç­›é€‰æŒ‡å®šç±»å‹çš„ç”¨æˆ·
        if request_type:
            filtered_ids = [
                user_id for user_id in self.user_ids 
                if self.user_vectors[user_id].request_type == request_type
            ]
            filtered_indices = [self.user_ids.index(uid) for uid in filtered_ids]
            filtered_vectors = self.vectors_matrix[filtered_indices]
        else:
            filtered_ids = self.user_ids
            filtered_vectors = self.vectors_matrix
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = cosine_similarity(filtered_vectors)
        
        return similarity_matrix, filtered_ids
    
    def add_user(self, user_id: str, text: str, request_type: str = "general", tags: List[str] = None, metadata: Dict = None) -> None:
        """æ·»åŠ å•ä¸ªç”¨æˆ·"""
        # è½¬æ¢ä¸ºå‘é‡
        vector = self.vectorizer.transform([text])[0]
        
        # åˆ›å»ºç”¨æˆ·å‘é‡å¯¹è±¡
        user_vector = UserVector(
            user_id=user_id,
            request_type=request_type,
            text=text,
            vector=vector,
            tags=tags or [],
            metadata=metadata or {}
        )
        
        self.user_vectors[user_id] = user_vector
        
        # æ›´æ–°å‘é‡çŸ©é˜µ
        self._update_vectors_matrix()
    
    def build_indices(self) -> None:
        """æ„å»ºç´¢å¼•ï¼Œæ›´æ–°å‘é‡çŸ©é˜µ"""
        self._update_vectors_matrix()
    
    def save_vectors(self, filepath: str) -> None:
        """ä¿å­˜ç”¨æˆ·å‘é‡åˆ°æ–‡ä»¶"""
        import pickle
        data = {
            'user_vectors': self.user_vectors,
            'user_ids': self.user_ids
        }
        with open(filepath, 'wb') as f:
            pickle.dump(data, f)
    
    def load_vectors(self, filepath: str) -> None:
        """ä»æ–‡ä»¶åŠ è½½ç”¨æˆ·å‘é‡"""
        import pickle
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
        
        self.user_vectors = data['user_vectors']
        self.users = self.user_vectors  # æ›´æ–°åˆ«å
        self.user_ids = data['user_ids']
        self._update_vectors_matrix()
    
    def _calculate_detailed_similarity(self, user_a_vector: UserVector, user_b_vector: UserVector):
        """è®¡ç®—è¯¦ç»†çš„ç”¨æˆ·ç›¸ä¼¼åº¦"""
        from dataclasses import dataclass
        
        @dataclass
        class SimilarityResult:
            overall_similarity: float
            vector_similarity: float
            profile_similarity: float
            request_similarity: float
            mutual_tags: List[str]
            overall_recommendation: str
            vector_explanation: str
        
        # è®¡ç®—å‘é‡ç›¸ä¼¼åº¦
        vector_sim = cosine_similarity(
            user_a_vector.vector.reshape(1, -1),
            user_b_vector.vector.reshape(1, -1)
        )[0][0]
        
        # è®¡ç®—æ ‡ç­¾ç›¸ä¼¼åº¦
        tags_a = set(user_a_vector.tags)
        tags_b = set(user_b_vector.tags)
        mutual_tags = list(tags_a.intersection(tags_b))
        
        if tags_a and tags_b:
            tag_similarity = len(mutual_tags) / len(tags_a.union(tags_b))
        else:
            tag_similarity = 0.0
        
        # è®¡ç®—è¯‰æ±‚ç›¸ä¼¼åº¦ï¼ˆåŸºäºæ–‡æœ¬ï¼‰
        if user_a_vector.text and user_b_vector.text:
            request_sim = cosine_similarity(
                self.vectorizer.transform([user_a_vector.text]),
                self.vectorizer.transform([user_b_vector.text])
            )[0][0]
        else:
            request_sim = vector_sim
        
        # ç»¼åˆè¯„åˆ†
        overall_sim = (vector_sim * 0.5 + tag_similarity * 0.3 + request_sim * 0.2)
        
        # ç”Ÿæˆæ¨èå’Œè§£é‡Š
        if overall_sim > 0.7:
            recommendation = "å¼ºçƒˆæ¨è"
            explanation = "ç”¨æˆ·æ¡£æ¡ˆé«˜åº¦ç›¸ä¼¼ï¼Œéå¸¸é€‚åˆåŒ¹é…"
        elif overall_sim > 0.5:
            recommendation = "æ¨è"
            explanation = "ç”¨æˆ·æ¡£æ¡ˆè¾ƒä¸ºç›¸ä¼¼ï¼Œå€¼å¾—è¿›ä¸€æ­¥äº†è§£"
        elif overall_sim > 0.3:
            recommendation = "ä¸€èˆ¬"
            explanation = "æœ‰ä¸€å®šç›¸ä¼¼æ€§ï¼Œå¯ä»¥è€ƒè™‘"
        else:
            recommendation = "ä¸æ¨è"
            explanation = "ç›¸ä¼¼åº¦è¾ƒä½ï¼ŒåŒ¹é…åº¦ä¸é«˜"
        
        return SimilarityResult(
            overall_similarity=overall_sim,
            vector_similarity=vector_sim,
            profile_similarity=tag_similarity,
            request_similarity=request_sim,
            mutual_tags=mutual_tags,
            overall_recommendation=recommendation,
            vector_explanation=explanation
        )
    
    def save_user_vectors(self, filepath: str) -> None:
        """ä¿å­˜ç”¨æˆ·å‘é‡æ•°æ®"""
        save_data = {}
        for user_id, user_vector in self.user_vectors.items():
            save_data[user_id] = {
                'user_id': user_vector.user_id,
                'request_type': user_vector.request_type,
                'text': user_vector.text,
                'vector': user_vector.vector.tolist(),
                'topics': user_vector.topics
            }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç”¨æˆ·å‘é‡å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_user_vectors(self, filepath: str) -> None:
        """åŠ è½½ç”¨æˆ·å‘é‡æ•°æ®"""
        with open(filepath, 'r', encoding='utf-8') as f:
            save_data = json.load(f)
        
        self.user_vectors = {}
        for user_id, data in save_data.items():
            user_vector = UserVector(
                user_id=data['user_id'],
                request_type=data['request_type'],
                text=data['text'],
                vector=np.array(data['vector']),
                topics=data['topics']
            )
            self.user_vectors[user_id] = user_vector
        
        self._update_vectors_matrix()
        logger.info(f"å·²ä» {filepath} åŠ è½½ {len(self.user_vectors)} ä¸ªç”¨æˆ·å‘é‡")

def demonstration_training_pipeline():
    """æ¼”ç¤ºå®Œæ•´çš„è®­ç»ƒå’ŒåŒ¹é…æµç¨‹"""
    
    print("ğŸš€ åŸºäºä¸»é¢˜å‘ç°çš„ç”¨æˆ·åŒ¹é…ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    training_texts = [
        "æˆ‘æ˜¯ä¸€ä¸ªç¨‹åºå‘˜ï¼Œå–œæ¬¢ç¼–ç¨‹å’ŒæŠ€æœ¯ï¼Œå¹³æ—¶çˆ±çœ‹ç§‘æŠ€æ–°é—»",
        "äº§å“ç»ç†ï¼Œå…³æ³¨ç”¨æˆ·ä½“éªŒå’Œå•†ä¸šæ¨¡å¼ï¼Œå–œæ¬¢åˆ†æå¸‚åœº",
        "è®¾è®¡å¸ˆï¼Œçƒ­çˆ±åˆ›æ„å’Œç¾å­¦ï¼Œç»å¸¸å…³æ³¨è®¾è®¡è¶‹åŠ¿",
        "åˆ›ä¸šè€…ï¼Œå¯¹å•†ä¸šå¾ˆæ„Ÿå…´è¶£ï¼Œå¸Œæœ›æ‰¾åˆ°æŠ€æœ¯åˆä¼™äºº",
        "æŠ•èµ„äººï¼Œå…³æ³¨æ—©æœŸé¡¹ç›®ï¼Œçœ‹å¥½AIå’ŒåŒºå—é“¾",
        "28å²å¥³ç”Ÿï¼Œå–œæ¬¢æ—…è¡Œå’Œæ‘„å½±ï¼Œå¸Œæœ›æ‰¾åˆ°å¿—åŒé“åˆçš„ä¼´ä¾£",
        "å†…å‘ç¨‹åºå‘˜ï¼Œå–œæ¬¢è¯»ä¹¦å’ŒéŸ³ä¹ï¼Œæƒ³æ‰¾ä¸ªç†è§£æˆ‘çš„äºº",
        "å¤–å‘é”€å”®ï¼Œçƒ­çˆ±ç¤¾äº¤å’Œè¿åŠ¨ï¼Œå¸Œæœ›æ‰¾ä¸ªå¼€æœ—çš„å¥³ç”Ÿ",
        "è‰ºæœ¯ç”Ÿï¼Œçƒ­çˆ±ç”»ç”»å’Œæ–‡å­¦ï¼Œæƒ³æ‰¾æœ‰è‰ºæœ¯ç»†èƒçš„äºº",
        "åŒ»ç”Ÿï¼Œå·¥ä½œå¿™ç¢Œä½†å¾ˆå……å®ï¼Œå¸Œæœ›æ‰¾ä¸ªä½“è´´çš„ä¼´ä¾£"
    ]
    
    # æ¨¡æ‹Ÿç”¨æˆ·æ•°æ®
    users_data = [
        {
            "user_id": "tech_001",
            "request_type": "æ‰¾é˜Ÿå‹",
            "text": "5å¹´ç»éªŒçš„å…¨æ ˆå·¥ç¨‹å¸ˆï¼Œæ“…é•¿Pythonå’ŒReactï¼Œæƒ³æ‰¾äº§å“åˆä¼™äººä¸€èµ·åšSaaS"
        },
        {
            "user_id": "biz_001", 
            "request_type": "æ‰¾é˜Ÿå‹",
            "text": "æœ‰è¿‡ä¸¤æ¬¡åˆ›ä¸šç»éªŒçš„äº§å“ç»ç†ï¼Œç†Ÿæ‚‰Bç«¯å¸‚åœºï¼Œå¯»æ‰¾æŠ€æœ¯åˆä¼™äºº"
        },
        {
            "user_id": "love_001",
            "request_type": "æ‰¾å¯¹è±¡",
            "text": "26å²ç¨‹åºå‘˜ï¼Œæ€§æ ¼å†…å‘ï¼Œå–œæ¬¢çœ‹ä¹¦å’Œå¬éŸ³ä¹ï¼Œå¸Œæœ›æ‰¾ä¸ªæ¸©æŸ”çš„å¥³ç”Ÿ"
        },
        {
            "user_id": "love_002",
            "request_type": "æ‰¾å¯¹è±¡", 
            "text": "25å²è®¾è®¡å¸ˆï¼Œçƒ­çˆ±æ—…è¡Œå’Œæ‘„å½±ï¼Œå¤–å‘å¼€æœ—ï¼Œæƒ³æ‰¾ä¸ªæœ‰å…±åŒçˆ±å¥½çš„ç”·ç”Ÿ"
        },
        {
            "user_id": "love_003",
            "request_type": "æ‰¾å¯¹è±¡",
            "text": "29å²åŒ»ç”Ÿï¼Œå·¥ä½œç¨³å®šï¼Œå–œæ¬¢è¿åŠ¨å’Œç¾é£Ÿï¼Œå¸Œæœ›æ‰¾ä¸ªç†è§£æˆ‘å·¥ä½œçš„äºº"
        }
    ]
    
    # æµ‹è¯•ä¸åŒçš„å‘é‡åŒ–æ–¹æ³•
    methods = ['tfidf', 'lda', 'doc2vec']
    
    for method in methods:
        print(f"\nğŸ“Š æµ‹è¯• {method.upper()} æ–¹æ³•")
        print("-" * 40)
        
        try:
            # 1. è®­ç»ƒå‘é‡åŒ–æ¨¡å‹
            if method == 'lda':
                vectorizer = TopicVectorizer(method, n_topics=5, max_features=500)
            else:
                vectorizer = TopicVectorizer(method)
            
            print(f"1. è®­ç»ƒ{method}æ¨¡å‹...")
            vectorizer.train(training_texts)
            
            # 2. åˆ›å»ºåŒ¹é…å™¨å¹¶æ·»åŠ ç”¨æˆ·
            matcher = VectorUserMatcher(vectorizer)
            print("2. æ·»åŠ ç”¨æˆ·å¹¶è®¡ç®—å‘é‡...")
            matcher.add_users(users_data)
            
            # 3. è¿›è¡Œç›¸ä¼¼åº¦åŒ¹é…
            print("3. æŸ¥æ‰¾ç›¸ä¼¼ç”¨æˆ·:")
            for user_id in ["tech_001", "love_001"]:
                similar_users = matcher.find_similar_users(user_id, top_k=3)
                print(f"\nä¸ {user_id} æœ€ç›¸ä¼¼çš„ç”¨æˆ·:")
                for sim_user_id, similarity in similar_users:
                    print(f"  â€¢ {sim_user_id}: {similarity:.3f}")
        
        except Exception as e:
            print(f"âŒ {method} æ–¹æ³•æµ‹è¯•å¤±è´¥: {e}")
    
    print(f"\nğŸ¯ æ¨èä½¿ç”¨æ–¹æ¡ˆ:")
    print("â€¢ TF-IDF: ç®€å•å¿«é€Ÿï¼Œé€‚åˆå°è§„æ¨¡æ•°æ®")
    print("â€¢ LDA: ä¸»é¢˜å»ºæ¨¡ï¼Œé€‚åˆå‘ç°æ½œåœ¨ä¸»é¢˜")  
    print("â€¢ Doc2Vec: æ·±åº¦è¯­ä¹‰ç†è§£ï¼Œé€‚åˆå¤æ‚æ–‡æœ¬")

if __name__ == "__main__":
    demonstration_training_pipeline() 